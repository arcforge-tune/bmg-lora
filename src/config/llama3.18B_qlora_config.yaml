model:
  model_id: meta-llama/Llama-3.1-8B-Instruct
  tokenizer_id: meta-llama/Llama-3.1-8B-Instruct
  pad_token_as_eos: true
  from_pretrained_params:
    load_in_low_bit: nf4              # 4‑bit NormalFloat quantization (QN4)
    optimize_model: false
    torch_dtype: bfloat16
    trust_remote_code: true
    modules_to_not_convert: ["lm_head"]
    use_cache: false
  gradient_checkpointing: true
  lora:
    enabled: true
    r: 16                               # per HF examples for 8B size :contentReference[oaicite:1]{index=1}
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
    bias: none
    task_type: CAUSAL_LM
    training_mode: qlora
  ipex:
    enabled: true
  prepare_kbit_training: true

data:
  dataset_type: json
  # dataset_name: yahma/alpaca-cleaned   # or any instruction‑style dataset
  data_files: data/train_data.json
  # split: train[:100]
  batch_size: 4                        # small batch due to model size
  max_length: 256
  # split_ratio: 0.9
  num_workers: 2
  instruction_style: SFT_llama

training:
  epochs: 3                            # common recommendation ≤3 for 8B :contentReference[oaicite:2]{index=2}
  learning_rate: 1e-5                 # from HF TRL guide :contentReference[oaicite:3]{index=3}
  weight_decay: 0.01
  gradient_accumulation_steps: 4
  save_steps: 100
  output_dir: outputs/lora_llama3_1_8b_instruct_xpu
  seed: 42
  device: xpu
  clear_xpu_cache: true

environment:
  ENABLE_ONEDNN_LAYOUT_OPT: "1"
  ONEDNN_DEFAULT_FPMATH_MODE: "BF16"
