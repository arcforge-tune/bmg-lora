logging:
  level: INFO
  log_dir: outputs/logs

data:
  dataset_type: json
  dataset_name: yahma/alpaca-cleaned
  split: train[:200]
  batch_size: 1
  max_length: 1024
  split_ratio: 0.9
  instruction_style: CLM_gpt2

model:
  model_id: gpt2
  tokenizer_id: gpt2
  pad_token_as_eos: true
  lora:
    enabled: true
    r: 4
    lora_alpha: 16
    target_modules: ["c_attn"]
    lora_dropout: 0.1
    bias: none
  ipex:
    enabled: true
    optimize_model: false

training:
  epochs: 5
  learning_rate: 0.0005
  output_dir: outputs/gpt2_lora_xpu
  save_model: true
  device: xpu  # "xpu" if available, else "cpu"
  clear_xpu_cache: true
  scheduler:
    min_lr: 1e-6
    max_lr: 5e-5
    warmup_steps: 50
    restart_interval: 100  # Add cycle length
    restart_mult: 1.0    # Add multiplier