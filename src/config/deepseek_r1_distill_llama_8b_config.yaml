model:
  model_id: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  tokenizer_id: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  pad_token_as_eos: true
  from_pretrained_params:
    load_in_low_bit: nf4              # 4‑bit NormalFloat quantization (QN4)
    optimize_model: false
    torch_dtype: bfloat16
    trust_remote_code: true
    modules_to_not_convert: ["lm_head"]
    use_cache: false
  gradient_checkpointing: true
  lora:
    enabled: true
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
    bias: none
    task_type: CAUSAL_LM
    training_mode: qlora
  ipex:
    enabled: true
    optimize_model: false
  prepare_kbit_training: true

data:
  dataset_type: json
  # dataset_name: yahma/alpaca-cleaned   # or any instruction‑style dataset
  data_files: data/datasets-autism.json
  split: train[:]
  batch_size: 1
  max_length: 3072
  num_workers: 16
  instruction_style: DeepSeek

training:
  epochs: 5
  learning_rate: 5e-5
  weight_decay: 0.02
  gradient_accumulation_steps: 4
  save_steps: 250
  output_dir: outputs/deepseek_r1_distill_llama_8b
  seed: 42
  device: xpu
  clear_xpu_cache: true
  scheduler:
    min_lr: 1e-6
    max_lr: 5e-5
    warmup_steps: 50
    restart_interval: 100
    restart_mult: 1.0

environment:
  CCL_PROCESS_LAUNCHER: none
  TORCH_LLM_ALLREDUCE: 1